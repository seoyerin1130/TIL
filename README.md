# TIL

## 순방향 신경망의 구조

![image](https://github.com/seoyerin1130/TIL/assets/127005534/011a02cb-838b-4244-bd54-69156d6fed53)

- 순방향 신경망은 위와 같이 뉴런들이 모여 Layer들을 이루고 계층이 쌓여
    
    전체 신경망을 이루는 구조 
    
- 순방향 신경망 모델은 데이터가 한 방향으로 전달되어 순방향 연결만 갖는 구조
- +
    
    순방향 신경망(fnn)과 달리 합성곱 신경망(cnn)은 공간 데이터를 가정하며 순환 신경망(rnn)은 순차 데이터를 가정함.
    

### 순방향 신경망의 계층 구조

![image](https://github.com/seoyerin1130/TIL/assets/127005534/c7486b79-238f-4c20-85ec-d0988aad1d9b)

순방향 신경망은 입력계층, 은닉계층, 출력계층으로 나뉜다.

입력 계층 : 데이터를 전달 받는다.

은닉 계층 : 데이터의 특징 추출 

출력 계층 : 특징을 바탕으로 추론한 결과를 출력


### 완전 연결 계층과 뉴런의 역할

- 순방향 신경망은 모든 계층이 **완전 연결 계층(fully connected layer)**으로 구성

**완전 연결 계층(fully connected layer)** 

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/b23b4232-5f1f-4c07-a365-b043a7ebd553/Untitled.png)

계층에 속한 각 뉴런이 이전 계층의 모든 뉴런과 모두 연결된 구조

**완전 연결 계층(fully connected layer)의 역할**

- 각 뉴런은 이전 계층에서 출력한 데이터를 동일하게 전달 받아
    
    **같은 입력 데이터에서 뉴런마다 서로 다른 특징을 추출한다.**
    
- 추측된 특징은 계층 단위로 출력되어 다음 계층에 한꺼번에 전달된다.
    - 데이터는 은닉 계층을 거친다.
    - 출력 계층은 가장 추상화된 특징을 이용해 예측한다.

### 특징을 추출하는 뉴런 구조

- 뉴런은 데이터에 내재한 특징을 추출하기 위해 **가중 합산과 활성 함수**를 순차적으로 실행

**가중 합산** : 추출할 특징에 영향을 미치는 데이터 선택 과정

**활성 함수** : 원하는 형태로 특징 추출하기 위해 데이터를 비선형적으로 변환하는 과정

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/e0e26c3f-d46a-4a5b-a9e5-30add966eb76/Untitled.png)

- 뉴런 구조는 퍼셉트론의 구조와 동일하다. Perceptron은 perception과 neuron의 합성어이며 인공 뉴런이라고도 부른다.

### 중요한 데이터를 선택하는 가중 합산 연산

뉴런에 입력 데이터 $x^T = (x_1, x_2, ..., x_n)$가 들어오면 가중치 $w^T = (w_1, w_2, ..., w_n)$와 곱해서 가중 합산을 한다.

                           $z = w_1x_1 + w_2x_2+...+w_nx_n+b=w^Tx+b$

가중치는 특징을 추출할 때 영향이 큰 데이터를 선택하는 역할을 한다. 

특징 추출에 **영향이 큰 데이터는 큰 가중치를** 갖고 **영향이 작은 데이터는 작은 가중치를** 가진다.

- 가중치
    
    **전체 집단에서 개별 구성요소가 차지하는 비중이나 중요도를 수치로 나타낸 값**
    

**이때 가중 합산식에 편향 b를 더하는 이유는 특징을 공간상 임의의 위치에 표현하기 위해서임**

만일 편향이 없다면 특정의 위치는 원점을 지나는 연속 함수로 결정되므로, 공간의 어느 위치에서든지 존재할 수 있게 하려면 편향으로 원점으로부터의 오프셋을 지정해 줘야 한다.

### 비선형 변환을 통해 특징을 추출하는 활성 함수

뉴런에 사용되는 함수 중 기본 활성 함수라고 할 수 있는 ReLU(Rectified Linear Unit)를 살펴보면     경첩 형태의 비선형 함수로 되어 있다.

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/ee0d0969-3d80-4b1e-a5ce-a675dbaadd1f/Untitled.png)

ReLU는 **입력 값이 0보다 크면 그대로 출력**하고 **0보다 작거나 같으면 0을 출력**하는 구간 선형 함수

**활성 함수=기저함수** : 데이터를 비선형 변환하여 원하는 형태의 특징을 추출할 수 있게 해준다.      신경망의 계층이 쌓이면서 뉴런의 활성 함수는 여러 단계로 합성되고 신경망이 표현하고자 하는    매우 복잡한 연속 함수나 결정 경계를 이루는 특징을 표현한다.

### 2.1.2 범용 함수 근사기로서의 신경망

---

**실함수와 벡터함수**

**실함수(real-valued function)** : $f: R^n$ → $R$ 형태의 함수이다. 

입력은 크기가   $n$인 백터 $x^T = (x_1, x_2, ..., x_n)$이고 출력은 실수로 다음과 같은 형태로 정의된다.
